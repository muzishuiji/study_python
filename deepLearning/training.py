# 神经网络的“学习”

# 神经网络的“学习”是指从训练数据中自动获取最优权重的参数的过程
# “学习”的目的就是以该损失函数为基准，找到能使它的值达到最小的权重参数


# 对于线形可分为题，是可以利用数据自动学习的
# 根据“感知机收敛定理”，通过有线次数的学习，线形可分问题是可解的
# 非线形可分问题则无法通过（自动）学习来解决

# 数据是机器学习的命根子，从数据中寻找答案，从数据中发现模式，根据数据讲故事，我发现我更适合做数据分析师

# 如何设计一个能将5正确分类的程序？
# 方案一：从图像中提取特征量，再用机器学习技术学习这些特征量的模式
# 这里所说的特征量是指了一从输入数据（输入图像）中准确提取本质数据的转换器。图像的特征量通常表示为向量的形式，
# 在计算机视觉领域，常用的特征量包括SIFT，SURF和HOG等，使用这些特征量将图像数据转换为向量，然后对转换后的向量是用机器学习中的SVM、KNN等分类器进行学习

# 机器学习能够更高效的帮助人们解决问题，但需要注意的是，将图像转换为向量事使用的特征量是由人设计的
# 对不同的问题，必须使用合适的特征量，才能得到更好的结果

# 深度学习有时也称为端到端学习（end to end machine learning）。这里所说的端到端氏之从一端到另一端的意识，也就是从原始数据（输入）中获得目标结果（输出）的意思

# 神经网络的优点是对所有的问题都可以用同样的流程来解决。识别5、识别狗或者识别人脸，与待处理的问题无关，设宁网络可以将数据直接作为原始数据，进行“端到端”的学习。

# 为了正确评价模型的泛化能力，需要划分训练数据和测试数据，训练数据可以被称为监督数据
# 泛化能力是指处理未被观察过的数据的能力，获得泛化能力是机器学习的最终目标。

# 只对某个数据集过度拟合的状态称为过拟合，避免过拟合也是机器学习的一个重要课题

# 神经网络总在以某个指标为线索寻找最优权重参数，该指标为损失函数（loss function）
# 这个损失函数可以使用任意函数，但一般为均访误差和交叉熵误差

# 均方误差的介绍
# yk表示神经网络的输出，tk表示监督数据，k表示数据的维数
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

# 用python来实现均方误差
import numpy as np
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t) ** 2)

# 测试一下，设2为正解
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
mean_squared_error(np.array(y), np.array(t))
# 设7为正解
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
mean_squared_error(np.array(y), np.array(t))
# 0.5975
# 结论：第一个例子的输出结果和监督数据更吻合

# 用代码实现交叉熵误差
def cross_entropy_error(y, t):
    delta = 1e-7
    # 加一个微小值delta防止负无限大导致溢出
    return -np.sum(t * np.log(y + delta))

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))
# 0.51082545709933802

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))
# 2.3025840929945458
# 结论：第一个例子的输出结果和监督数据更吻合


# 训练数据有n个，要把这n个损失函数的总和作为学习的指标
# 神经网络的学习是从训练数据中选出一批数据（称为mini-batch，小批量），然后对每个mini-batch进行学习

# 随机选择指定个数的数据的代码，以进行mini-batch学习
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

print(x_train.shape) # (60000, 784)
print(t_train.shape) # (60000, 10)

# 从训练数据中随机抽取10个数据
train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]

np.random.choice(60000, 10)
# array([33597, 29487, 30348, 30337, 53445, 36703, 37307, 34465, 49443, 44780])

# mini-batch的损失函数用随机选择的小批量数据作为全体训练数据的近似值
# 可同时处理单个数据和批量数据的交叉熵误差
def cross_entropy_error(y, t):
    if y.ndim == 1:
        # y的维度为1时，需要改变数据的形状
        t = t.reshape(1, t.size)
        y = y.rehape(1, y.size)
    batch_size = y.shape[0]
    # 用batch的个数进行正规化
    return -np.sum(t * np.log(y + 1e-7)) / batch_size

# 当监督数据时标签形式（非one-hot表示，而是像2，7这样的标签时），交叉熵误差可通过如下代码实现
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.array(batch_size), t] + 1e-7)) / batch_size
# ont-hot表示中t为0的元素的交叉熵误差也为0，因此针对这些元素的计算可以忽略
# 如果可以获得神经网络在正确解标签出的输出，就可以计算交叉熵误差
# np.array(batch_size)：生成一个0-batch_size-1的数组，如[0,1,2,3,4,5,6,7,8,9]
# t中标签是以[2,7,0,9,4]的形式存储的，所以y[np.array(batch_size), t]就能抽出各个数据的正确解标签对应的神经网络的输出
# 在这个例子中y[np.array(batch_size), t]会生成numpy数组[y[0,2], y[1,7],y[2,0],y[3,9],y[4,4]]


# 进行神经网络的学习时，不能讲识别精度作为指标，因为如果以识别精度为指标，则参数的导数在绝大多书地方都会变为0
# 识别精度对微小的参数变化基本上没有反应，即便有反应，它的值也是不连续的，突然的变化，作为激活函数的阶跃函数也有同样的问题
# 如果使用阶跃函数作为激活函数，神经网络的学习将无法进行，参数的微小变化会被阶跃函数抹杀，导致损失函数的值不会发生任何变化
# 单如果把损失函数作为指标，当前损失函数的值会随着参数的值的变化产生连续性的变化

# 阶跃函数像“竹筒敲石”一样，只在某个瞬间发生变化，
# 而sigmoid函数，不仅函数的输出时连续变化的，曲线的斜率也是连续变化的，sigmoid函数的导数在任何地方都不为0，这对神经网络的学习非常重要
# 得益于这个斜率不为0的性质，神经网络的学习得以正确进行