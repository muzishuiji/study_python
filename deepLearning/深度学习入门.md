## 前言


## 第一章

## 第二章

## 第三章


## 第四章

- 机器学习中使用的数据集分为训练数据和测试数据；
- 神经网络用测试数据进行学习，并用测试数据评价学习到的模型的泛化能力；
- 神经网络的学习以损失函数为指标，更新权重参数，以使损失函数的值减小；
- 利用某个给定的微小值的差分求导的过程，称为数值微分；
- 利用数值微分，可以计算权重参数的梯度；
- 数值微分虽然费时间，但是实现起来很简单。稍微复杂一些的误差反向传播法可以高速地计算梯度；

## 第五章

### 误差反向传播法
正确理解误差反向传播法，有两种方法：一种是基于数学式，另一种是基于计算图。

通过计算图来理解误差反向传播法这个想法，参考了Andrej Karpathy的博客[4]和他与Fei-Fei Li教授负责的斯坦福大学的深度学习课程CS231n

- 正向传播：
正向传播是从计算图出发点到结束点的传播。
- 反向传播
反向传播就是从计算图的结束点到出发点的传播。
- 局部计算
计算图将复杂的计算方式分割成简单的局部计算，和流水线作业一样，将局部计算的结果传递给下一个节点。
- 使用计算图的原因
使用计算图，可以通过正向传播和反向传播高效计算各个变量的导数值。

### 链式法则

如果某个函数由符合函数表示，则该复合函数的导数可以用构成符合函数的某个函数的导数的乘积表示。

### 反向传播

1. 加法的反向传播是将上游的值原封不动的传输到下游；
2. 乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传输给下游。

z = xy，正向传播时信号是x的话，反向传播时则是y；正向传播时信号是y的话，反向传播时则是x。

加法的反向传播只是将上游的值传给下游，并不需要正向传播的输入信号。但是，乘法的反向传播需要正向传播时的输入信号值。因此，实现乘法节点的反向传播时，要保存正向传播的输入信号。


### 简单层的实现
1. 计算图的乘法节点称为“乘法层”，加法节点称为“加法层”。
2. 我们把构建神经网络的“层”实现为一个类，这里所说的“层”时神经网络中功能的单位。比如，负责sigmoid函数的sigmoid、负责矩阵乘积的affine等，都是以层为单位进行实现。

## 函数激活层的实现

1. 将构成的神经网络的层实现为一个类，实现激活函数Relu层和Sigmoid层。
2. Relu层的作用就像电路的开关一样，正向传播时，有电流通过的话，就将开发设为ON。没有电流通过的话，就将开发设为OFF。反向传播时，开关为ON的话，电流会直接通过。开关为OFF的话，则不会有电流通过。
3. 矩阵的乘积运算中对应维度的个数要保持一致。

X        *      W     =     O
(2,)          (2,3)        (3,)

4. 神经网络的正向传播中进行的矩阵的乘积运算在几何领域被称为“仿射变换”。神经网络里将仿射变换的处理实现为Affine层。

5. 几何中，仿射变换包括一次性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。

6. 矩阵的乘积（dot节点）的反向传播可以通过组建使矩阵对应维度的元素个数一致的乘积运算而推导出来。

7. 神经网络中进行的处理有推理和学习两个阶段。神经网络的推理通常不使用Softmax层。神经网络进行推理时，会将最后一个affine层的输出作为识别结果。神经网络中未被正规化的输出结果有时称为“得分”。当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要Softmax层，神经网络的学习阶段需要Softmax层。

8. 使用交叉熵误差作为Softmax函数的损失函数后，反向传播得到(y1 - t1, y2 - t2, y3 - t3)这样“漂亮”的结果。实际上，这样“漂亮”的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。回归问题中输出层使用“恒等函数”，损失函数使用“平方和误差”，也是出于同样的理由。也就是说，使用“平方和误差”作为“恒等函数”的损失函数，反向传播才能得到(y1 - t1, y2 - t2, y3 - t3)这样“漂亮”的结果。

9. 微分差值法通常用来衡量误差反向传播法计算结果的准确性，这一过程也称为梯度确认。

数值微分和误差反向传播法的计算结果之间的误差为0是很少见的。这是因为计算机的计算精度有限（比如，32位浮点数）。受到数值精度的限制，两者的误差一般不会为0，如果实现正确的变化，这个误差是一个接近0的很小的值。如果这个值很大，则说明误差反向传播法的实现存在错误。

### 第五章总结

1. 以层为单位实现神经网络中的处理，如ReLU层，softmax-with-loss层、Affine层等，这些层实现了forward和backward方法，通过将数据正向和反向的传播，可以高效的计算权重参数的梯度。
2. 通过使用层进行模块化，神经网络中可以自由的组装层，轻松的构建出自己喜欢的网络。
3. 使用计算图，可以直观的把握计算过程。
4. 计算图的节点是由局部计算构成的，局部计算构成全局计算。
5. 通过比较数值微分和误差反向传播法的结果，可以确认误差反向传播法的实现是否正确（梯度确认）。

## 第六章 与学习相关的技巧
### 参数的更新
1. 神经网络学习的目的是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）

2. 通常，我们使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法（SGD， stochastic gradient descent）。

3. 如果函数的形状非均向，比如呈延伸状，搜索的路径就会非常低效。我们需要比单纯朝梯度方向前进的SGD更聪明的方法，SGD低效的根本原因是，梯度的方向并没有指向最小值的方向。
4. Momentum在SGD的基础上引入动量，一定程度上减弱了搜索过程中的波动，从而参数优化路径更快的收敛（学习的更快）。

5. AdaGrad会记录所有梯度的平方和。学习越深入，更新的幅度就越小。实际上，如果无止境的学习，更新量就会变为0，完全不再更新。为了改善这个问题，可以使用RMSRrop方法，RMSRrop方法并不是将过去的所有的梯度一视同仁的想家，而是逐渐的遗忘过去的梯度，在做加法运算时将新梯度的信息更多的反映出来。这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度。

6. Momentum参照小球在碗中滚动的物理规则进行移动（增加摩擦阻力），AdaGrad为参数的每个元素适当的调整更新步伐。Adam方法的基本思路就是将这两个方法融合，实现参数空间的高效搜索。

7. Adam方法和AdaGrad方法的学习效果类似，但是实验结果会随着学习率等超参数、神经网络的结构（几层深）的不同而发生变化。

### 超参数

超参数（hyperparameters）是指在机器学习模型训练过程中，需要事先设定的参数，而不是通过训练数据学习的到的参数。超参数主要有以下几个方面的作用：
1. 控制模型复杂度：通过调整超参数，可以改变模型的复杂度，从而防止过拟合和欠拟合。
2. 控制模型收敛速度：一些超参数直接影响模型的训练效率和收敛速度。
3. 影响模型性能：不同的超参数组合会影响模型的预测性能，因此需要进行调优以找到最优的组合。

### 权重的初始值

1. 为了防止“权重均一化”（严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值。
2. 偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失。这个问题称为梯度消失（gradient vanishing）。层次加深的深度学习中，梯度消失的问题可能会更加严重。
3. 如果多个神经元都输出几乎相同的值，则说明激活值的分布有所偏向，比如，100个神经元都输出几乎相同的值，那么也可以由1个神经元来表达基本相同的事情。因此，激活值在分布上有所偏向会出现“表现力受阻”的问题。
4. 各层的激活值分布都要求有适当的广度。因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受阻”的问题，导致学习可能无法顺利进行。
5. 当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用xavier初始值。
6. 在神经网络的学习中，权重初始值非常重要。很多时候权重初始值的设定关系到神经网络的学习是否成功。
7. 作为权重初始值，Xavier初始值，He初始值等比较有效；

### batch normalization

1. 设定合适的权重值，则各层的激活值分布会有适当的广度。
2. Batch Norm的优点：
  - 可以快速进行学习（增大学习率）；
  - 不那么依赖初始值（不那么依赖设置合理的初始值）；
  - 抑制过拟合（降低Dropout等的必要性）；

  ### 正则化

1. 过拟合的原因：
    - 模型拥有大量参数，表现力强；
    - 训练数据少；

2. 权值衰减
权值衰减是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。

对于所有权重，权值衰减方法都会为损失函数加上1/2入W^2

3. L2范数

L2范数相当于各个元素的平方和。用数学式表示的话，假设有权重W=(w1,w2,...,wn)，则L2范数可用 根号w1^2+w2^2+w3^2+...+wn^2计算出来。L1范数就是各个元素的绝对值之和，相当于|w1|+|w2|+...+|wn|,L∞范数也称为Max范数，相当于各个元素的绝对值中最大的那一个。L2范数，L1范数，L∞范数都可以用作正则化项，它们各有各的特点。

3. Dropout方法

L2范数在某种程度上能够抑制过拟合，但如果网络的模型变得复杂，只用权值衰减就难以应付了，这种情况下，会使用Dropout方法。

通过使用Dropout，即便表现力强的网络，也可以抑制过拟合。
机器学习中经常使用集成学习。所谓集成学习，就是让多个模型单独进行学习，推理时再取多个模型输出的平均值。

Dropout可以理解为，通过在学习过程中删除神经元，从而每一次都让不同的模型进行学习。并且，推理时，通过对神经元的输出乘以删除比例（比如0.5），可以取得模型的平均值。Dropout将集成学习的效果（模拟地）通过一个网络实现了。

4. 训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（比较理想的是只用一次）测试数据。

5. 有报告显示，在进行神经网络的超参数的最优化时，与王哥搜索等有规律的搜索相比，随机采样的搜索方式效果更好。这是因为在多个超参数重，各个超参数对最终识别精度的影响程度不同。

6. 超参数优化的步骤
  - 步骤0: 设定超参数的范围；
  - 步骤1: 从设定的超参数范围中随机采样；
  - 步骤2: 使用步骤1采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将epoch设置的很小）；
  - 步骤3: 重复步骤1和步骤2，根据它们的识别精度的结果，缩小超参数的范围；

7. 上述的超参数的最优化方法时实践性的方法，不过也是实践者的经验的感觉。在超参数的最优化中，如果需要更精炼的方法，可以使用贝叶斯最优化。贝叶斯最优化运用以贝叶斯定理为中心的数学理论，能够更加严密、高效地进行最优化。

8. 逐渐缩小“好值”存在的范围时搜索超参数的一个有效方法。


## 第七章 卷积神经网络

1. 卷积神经网络（Convolutional Neural Network），CNN被用于图像识别、语音识别等各种场合，在图像识别的比赛中，基于深度学习的方法几乎都以CNN为基础。

2. 卷积运算的填充处理：使用填充主要是为了调整输出的大小。比如，对大小为(4, 4)的输入数据应用(3,3)的滤波器时，输出大小变为(2,2)，相当于输出大小比输入大小缩小了2个元素。这在反复多次进行卷积运算的深度网络中会成为问题。为什么呢？因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为1，导致无法再应用卷积运算，为了避免出现这样的情况，就要使用填充，在刚才的例子中，将填充的幅度设为1，那么相对于输入大小(4,4)，输出大小也保持为原来的(4,4)。因此，卷积运算就可以保持在空间大小不变的情况下将数据传给下一层。
对卷积运算做批处理，可以将N次的处理汇总成1次进行。















## 总结